<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenzheng Zeng</title>

    <meta name="author" content="Wenzheng Zeng">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wenzheng Zeng ÊõæÊñáÊ≠£
                </p>
                <p>I am a first year PhD student at National University of Singapore (NUS). I received my bachelor and master degree from Huazhong University of Science and Technology (HUST).
                </p>
                <p>
                  My research interests include computer vision and pattern recognition, particularly in detection and segmentation in both space and time. Recently, I work on video-language modeling.
                </p>
                <p style="text-align:center">
                  <a href="mailto:wenzhengzeng@hust.edu.cn">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com.sg/citations?hl=en&user=RDTJO-4AAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/wenzhengzeng">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/zwz2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zwz2.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    


            <tr onmouseout="mcgaze_stop()" onmouseover="mcgaze_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                
                <img src="images/mcgaze_demo.gif" width="180">
            
                
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                
                <span class="papertitle">End-to-End Video Gaze Estimation via Capturing Head-Face-Eye Spatial-Temporal Interaction Context</span>
                <br>
                <span><strong>End-to-End Video Gaze Estimation via Capturing Head-Face-Eye Spatial-Temporal Interaction Context</strong></span>
                <br>
                Yiran Guan*,
                Zhuoguang Chen*,
                
                <u><b>Wenzheng Zeng<sup>#</sup></b></u>,
                Zhiguo Cao,
                Yang Xiao<sup>#</sup></a>,
                <br>
                (*: equal contribution;  #: corresponding author)
                <br>
                <em>IEEE Signal Processing Letters</em>, 2023
                <br>
                <a href="https://github.com/zgchen33/MCGaze">project page & code</a>
                /
                <a href="https://arxiv.org/abs/2310.18131">paper</a>
                
                <p></p>
              </td>
            </tr> 
      
    


              
            <tr onmouseout="mpeblink_stop()" onmouseover="mpeblink_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                
                <img src="images/mpeblink_demo.gif" width="180">
            
                
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://wenzhengzeng.github.io/mpeblink">
                  <span class="papertitle">Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video</span>
                </a>
                <br>
                <strong>Wenzheng Zeng</strong>,
                <a href="https://scholar.google.com/citations?user=NeKBuXEAAAAJ&hl=en">Yang Xiao</a>,
                Sicheng Wei,
                Jinfang Gan,
                Xintao Zhang,
                <a href="https://scholar.google.com/citations?hl=en&user=396o2BAAAAAJ">Zhiguo Cao</a>,
                <a href="https://scholar.google.com/citations?user=UX5N_FQAAAAJ&hl=en">Zhiwen Fang</a>,
                <a href="https://joeyzhouty.github.io/">Joey Tianyi Zhou</a>
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                <br>
                <a href="https://wenzhengzeng.github.io/mpeblink">project page</a>
                /
                <a href="https://wenzhengzeng.github.io/mpeblink/static/images/mpeblink.pdf">paper</a>
                /
                <a href="https://www.youtube.com/embed/ngME7dym0Uk">video</a>
                /
                <a href="mpeblink/static/images/cvpr23_poster.pdf">poster</a>
                /
                <a href="https://github.com/wenzhengzeng/MPEblink">code</a>
                /
                <a href="https://zenodo.org/record/7754768">dataset</a>
                
                <p></p>
              </td>
            </tr> 

            <tr onmouseout="blink_eyelid_stop()" onmouseover="blink_eyelid_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
               
                  <img src='images/blink_eyelid.png' width="180">
                
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://github.com/wenzhengzeng/blink_eyelid">
                  <span class="papertitle">Eyelid‚Äôs Intrinsic Motion-aware Feature Learning
                    for Real-time Eyeblink Detection in the Wild</span>
                </a>
                <br>
                <strong>Wenzheng Zeng</strong>,
                <a href="https://scholar.google.com/citations?user=NeKBuXEAAAAJ&hl=en">Yang Xiao</a>,
                Guilei Hu,
                <a href="https://scholar.google.com/citations?hl=en&user=396o2BAAAAAJ">Zhiguo Cao</a>,
                Sicheng Wei,
                <a href="https://scholar.google.com/citations?user=UX5N_FQAAAAJ&hl=en">Zhiwen Fang</a>, <br>
                <a href="https://joeyzhouty.github.io/">Joey Tianyi Zhou</a>,
                <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>
                <br>
                <em>IEEE Transactions on Information Forensics and Security (TIFS)</em>, 2023
                <br>
                
                
                <a href="data/blink_eyelid_final.pdf">paper</a>
                /
                <a href="https://github.com/wenzhengzeng/blink_eyelid">code</a>
                <p></p>
              </td>
            </tr>



           

    

          </tbody></table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Invited Talk</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gaze2023_low_res.png", width="180"></td>
              <td width="75%" valign="center">
                <span><strong>Invited poster and spotlight talk at <a href="https://gazeworkshop.github.io/2023/">CVPR GAZE2023 workshop</a>.</strong></span>
          <br>
                Topic: Multi-person eyeblink detection in the wild in untrimmed videos
                <br>
                
              </td>
            </tr>
        
            
            
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Competitions</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/logo_new.gif", width="180"></td>
              <td width="75%" valign="center">
                <span><strong>The "Challenge Cup" National Competition, 2023</strong></span>
          <br>
      
                <strong>Topic:</strong> Driver Monitoring System (DMS).
                <br>
                <strong>Result:</strong> Winning the <strong>Grand Prize (top prize beyond First Prize)</strong>.
                <br>
                
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ECCV-logo3.png", width="180"></td>
              <td width="75%" valign="center">
                <span><strong>The Visual Inductive Priors for Data-Efficient Computer Vision Challenge, ECCV 2022</strong></span>
          <br>
                
                <strong>Topic:</strong> Action recognition with limited training data.
                <br>
                <strong>Result:</strong> Obtain <strong>3rd place</strong> and <strong>Jury Prize</strong> in the action recognition track.
                <br>
                
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Parkinson_competition.jpg", width="180"></td>
              <td width="75%" valign="center">
                <span><strong>China Graduate AI Innovation Competition, 2021</strong></span>
          <br>
                
                <strong>Topic:</strong> Cloud + AI assist Parkinson's Diagnosis.
                <br>
                <strong>Result:</strong> Obtain <strong>3rd place and fisrt prize (3/1505)</strong>.
                <br>
                
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ICCV logo_Homepage_sm.png", width="180"></td>
              <td width="75%" valign="center">
                <span><strong>Fisheye Video-based Action Recognition Competition at the MMVRAC Workshop, ICCV 2021</strong></span>
          <br>
                
                <strong>Topic:</strong> Action recognition in fisheye videos.
                <br>
                <strong>Result:</strong> Obtain <strong>4th place</strong>.
                <br>
                
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/COMAP-logo-EF3340.svg", width="170"></td>
              <td width="75%" valign="center">
                <span><strong>The Mathematical Contest In Modeling (MCM), America, 2020</strong></span>
          <br>
                
                <strong>Topic:</strong> Comprehensive exploration of buyer reviews and ratings for Amazon e-commerce products.
                <br>
                <strong>Result:</strong> Obtain <strong>Meritorious Winner (9%)</strong>.
                <br>
                
              </td>
            </tr>


            
          </tbody></table>


          <p></p><p></p><p></p><p></p><p></p>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                  <td width="100%" valign="middle">
                    <h2>Selected Honors & Scholarships</h2>
                    <div style="line-height:25px">
                    <p>
                
                <li> <stronghuge>National Scholarship (Rank 1/600+),&nbsp; 2023<br/>
                <li> <stronghuge>HUAWEI Scholarship (Rate < 1%), &nbsp; 2023<br/>
                <li> <stronghuge>Outstanding Graduate,&nbsp; 2021<br/>
                <li> <stronghuge>Excellent Undergraduate Thesis </stronghuge> (Rate < 3% in HUST),&nbsp; 2021<br/>
  
                    </p>
                    </div>
                  </td>
                </tr>
          </table>









          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is inspired by <a href="https://jonbarron.info/">Jon Barron's website</a>. Many thanks to him!
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
