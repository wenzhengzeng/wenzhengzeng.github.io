<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenzheng Zeng</title>

    <meta name="author" content="Wenzheng Zeng">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="/images/logo/18.png">

    <style>
      body {
        line-height: 1.55;  /* 调大整体行间距，默认大概是1.4~1.5 */
      }
      p, li {
        line-height: 1.6; /* 段落、列表再稍微宽松 */
      }
    </style>

    <style>
      body {
        font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;
      }
    </style>

    
  </head>

  <body>
    <!-- <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:0.5%;width:72%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  <!-- <span style="font-family: 'Times New Roman', Times, serif;">Wenzheng Zeng</span>  -->
                  Wenzheng Zeng   「曾文正」
                  <!-- <span style="font-family: 'SimSun', '宋体', serif;">曾文正</span> -->
                </p>
              </td>
              <!-- <td style="padding:0.5%;width:30%;vertical-align:middle">
              </td> -->
            </tr>
            <tr style="padding:0px">
              <td style="padding:0.5%;width:72%;vertical-align:middle">   
                <p>I am a first year PhD candidate at the National University of Singapore (NUS), 
                  affiliated with the <a href="https://www.comp.nus.edu.sg/">Department of Computer Science</a> 
                  and <a href="https://sites.google.com/view/showlab/home">Show Lab</a>, 
                  co-supervised by
                  <a href="https://scholar.google.com/citations?user=FABZCeAAAAAJ&hl">Prof. Hwee Tou Ng</a>
                  and
                  <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike Zheng Shou</a>.
                  I received my B.Eng and M.Phil degree from
                  <a href="https://aia.hust.edu.cn">School of Artificial Intelligence and Automation</a>,
                  <a href="https://english.hust.edu.cn">Huazhong University of Science and Technology (HUST)</a>, working with 
                  <a href="https://scholar.google.com/citations?user=NeKBuXEAAAAJ&hl">Prof. Yang Xiao</a>
                  and
                  <a href="https://scholar.google.com/citations?user=396o2BAAAAAJ&hl">Prof. Zhiguo Cao</a>, and also collaborating with
                  <a href="https://scholar.google.com/citations?user=cYNqDokAAAAJ&hl">Dr. Joey Tianyi Zhou</a>
                  and
                  <a href="https://scholar.google.com/citations?hl=en&user=fJ7seq0AAAAJ">Prof. Junsong Yuan</a>.

                </p>
                <p>
                  I work on computer vision research, with a primary focus on multimodal and spatiotemporal intelligence.
                </p>
                <!-- <p>
                  My hust.edu.cn email has expired since 2024.9. Feel free to contact me via [wenzhengzeng@u.nus.edu] or [zengwenzheng126@126.com].
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:wenzhengzeng@u.nus.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.sg/citations?hl=en&user=RDTJO-4AAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/wenzhengzeng">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.zhihu.com/people/alex-31-36-62">知乎</a>
                  
                </p>
                <p style="text-align:center">
                  <a href="https://twitter.com/alexzeng1206?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @alexzeng1206</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>  
                </p>



                
              </td>
              <td style="padding:0.5%;width:28%;vertical-align:top">
                <a href="images/zwz3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zwz3.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Publications <small>(*: equal contribution, <span class="supmark"></span>&dagger;</span>: project lead, <span class="supmark"></span>&Dagger;</span>: corresponding author)</small></h2>

                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="slidetailor_stop()" onmouseover="slidetailor_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/slidetailor.png" style="width:95%;">
            
                
              </td>
             
              <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</span>
                <br>

                <u><b>Wenzheng Zeng*</b></u>,
                Mingyu Ouyang*,
                Langyuan Cui*,
                Hwee Tou Ng<span class="supmark_1">&Dagger;</span>
                <br>
                <em>AAAI 2026</em>
                <br>

                <a href="https://arxiv.org/abs/2512.20292">[Paper]</a>
                <a href="https://github.com/nusnlp/SlideTailor">[Code]</a>
                <a href="https://drive.google.com/drive/folders/1N8p1A4eW8Nrrc2fN5NnIutG0og9u_GIy">[Poster]</a>
                <a href="https://www.youtube.com/watch?v=NT5kWE6j_Vw">[Video]</a>
                <a href="https://drive.google.com/drive/folders/1N8p1A4eW8Nrrc2fN5NnIutG0og9u_GIy">[Slides]</a>
                <p></p>
              </td>
            </tr> 


            <tr onmouseout="d2vlm_stop()" onmouseover="d2vlm_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/d2vlm/1.png" style="width:95%;">
            
                
              </td>
             
              <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">Factorized Learning for Temporally Grounded Video-Language Models</span>
                <br>

                <u><b>Wenzheng Zeng</b></u>,
                Difei Gao,
                Mike Zheng Shou<span class="supmark_1">&Dagger;</span>,
                Hwee Tou Ng<span class="supmark_1">&Dagger;</span>
                <br>
                <em>ICCV 2025</em>
                <br>

                <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zeng_Factorized_Learning_for_Temporally_Grounded_Video-Language_Models_ICCV_2025_paper.pdf">[Paper]</a>
                <a href="https://openaccess.thecvf.com/content/ICCV2025/supplemental/Zeng_Factorized_Learning_for_ICCV_2025_supplemental.pdf">[Supp.]</a>
                <a href="https://github.com/nusnlp/d2vlm">[Code]</a>
                <a href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/1301.png?t=1759990615.5445755">[Poster]</a>
                <a href="https://www.youtube.com/watch?v=DylkFjyTITs&t=2s">[Video]</a>
                <a href="https://iccv.thecvf.com/virtual/2025/poster/1301">[Project]</a>
                <p></p>
              </td>
            </tr> 
          
            

            <tr onmouseout="mpmat_stop()" onmouseover="mpmat_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/mpmat/mpmat_fig1.png" style="width:95%;">
            
                
              </td>
             
              <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">MP-Mat: A 3D and Instance-Aware Human Matting and Editing Framework with Multiplane Representation</span>
                <br>

                Siyi Jiao*,
                <u><b>Wenzheng Zeng*<span class="supmark_1">&dagger;</b></u>,
                Yerong Li,
                Huayu Zhang,
                Changxin Gao,
                Nong Sang<span class="supmark_1">&Dagger;</span>,
                Mike Zheng Shou<span class="supmark_1">&Dagger;</span>
                <br>
                <em>ICLR 2025</em>
                <br>

                <a href="https://arxiv.org/pdf/2504.14606">[Paper]</a>
                <a href="https://github.com/JiaoSiyi/MPMat">[Code]</a>
                
                
                <p></p>
              </td>
            </tr> 

            <tr onmouseout="dfimat_stop()" onmouseover="dfimat_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/dfimat/2.png" style="width:95%;">
            
                
              </td>
             
                <td style="padding:10px 10px;width:75%;vertical-align:middle">

                
                <span class="papertitle" style="font-size: 1.06em;">DFIMat: Decoupled Flexible Interactive Matting in Multi-Person Scenarios</span>
                <br>

                Siyi Jiao*,
                <u><b>Wenzheng Zeng*</b></u>,
                Changxin Gao,
                Nong Sang,
                <br>
                <em>ACCV 2024</em>
                <br>

                <a href="https://arxiv.org/pdf/2410.09788">[Paper]</a>
                <a href="https://github.com/JiaoSiyi/DFIMat">[Code]</a>
                
                
                <p></p>
              </td>
            </tr> 

            <tr onmouseout="crossglg_stop()" onmouseover="crossglg_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/cross_glg.png" style="width:95%;">
            
              
              </td>
               <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner</span>
                <br>

                Tingbing Yan,
                <u><b>Wenzheng Zeng<span class="supmark_1">&Dagger;</span></b></u>,
                Yang Xiao<span class="supmark_1">&Dagger;</span>,
                Xingyu Tong,
                Bo Tan,
                Zhiwen Fang,
                Zhiguo Cao,
                Joey Tianyi Zhou
                <br>
                <em>ECCV 2024</em>
                <br>

                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03008.pdf">[Paper]</a>
                <a href="https://github.com/Baron-sanmen/CrossGLG">[Code]</a>
                
                
                <p></p>
              </td>
            </tr> 


            <tr onmouseout="mcgaze_stop()" onmouseover="mcgaze_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/mcgaze_demo.gif" style="width:95%;">
            
                
              </td>
              <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">End-to-End Video Gaze Estimation via Capturing Head-Face-Eye Spatial-Temporal Interaction Context</span>
                <br>
                <!-- <span><strong>End-to-End Video Gaze Estimation via Capturing Head-Face-Eye Spatial-Temporal Interaction Context</strong></span>
                <br> -->
                Yiran Guan*,
                Zhuoguang Chen*,
                
                <u><b>Wenzheng Zeng<span class="supmark_1">&Dagger;</span></b></u>,
                Zhiguo Cao,
                Yang Xiao<span class="supmark_1">&Dagger;</span></a>
                <br>
                <em>IEEE Signal Processing Letters</em>, 2023
                <br>
                
                <a href="https://arxiv.org/abs/2310.18131">[Paper]</a>
                <a href="https://github.com/zgchen33/MCGaze">[Code]</a>
                
                <p></p>
              </td>
            </tr> 
      
    

              
            <tr onmouseout="mpeblink_stop()" onmouseover="mpeblink_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
                
                <img src="images/mpeblink_demo.gif" style="width:95%;">
            
                
              </td>
              <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video</span>
                <!-- <br> -->
                <!-- <br style="margin-bottom: 1.2em;"> -->
                <div style="margin-bottom: 0.2em;"></div>
                <u><b>Wenzheng Zeng</b></u>,
                Yang Xiao,
                Sicheng Wei,
                Jinfang Gan,
                Xintao Zhang,
                Zhiguo Cao,
                Zhiwen Fang,
                Joey Tianyi Zhou
                <br>
                <em>CVPR 2023</em>
                <br>
                <a href="https://wenzhengzeng.github.io/mpeblink">[Project Page]</a>
                
                <a href="https://wenzhengzeng.github.io/mpeblink/static/images/mpeblink.pdf">[Paper]</a>
                
                <a href="https://www.youtube.com/embed/ngME7dym0Uk">[Video]</a>
                
                <a href="mpeblink/static/images/cvpr23_poster.pdf">[Poster]</a>
              
                <a href="https://github.com/wenzhengzeng/MPEblink">[Code]</a>
                
                <a href="https://zenodo.org/record/7754768">[Dataset]</a>
                
                <p></p>
              </td>
            </tr> 

            <tr onmouseout="blink_eyelid_stop()" onmouseover="blink_eyelid_start()">
              <td style="padding:10px 10px;width:25%;vertical-align:middle">
               
                  <img src='images/blink_eyelid_new.png' style="width:95%;">
                
              </td>
              <td style="padding:10px 10px;width:75%;vertical-align:middle">
                
                <span class="papertitle" style="font-size: 1.06em;">Eyelid’s Intrinsic Motion-aware Feature Learning
                    for Real-time Eyeblink Detection in the Wild</span>
                <div style="margin-bottom: 0.2em;"></div>
                <u><b>Wenzheng Zeng</b></u>,
                Yang Xiao,
                Guilei Hu,
                Zhiguo Cao,
                Sicheng Wei,
                Zhiwen Fang, <br>
                Joey Tianyi Zhou,
                Junsong Yuan
                <br>
                <em>IEEE Transactions on Information Forensics and Security (TIFS)</em>, 2023
                <br>
                
                
                <a href="data/blink_eyelid_final.pdf">[Paper]</a>
                
                <a href="https://github.com/wenzhengzeng/blink_eyelid">[Code]</a>
                <p></p>
              </td>
            </tr>



           

    

          </tbody></table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Invited Talk</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:5px 20px;width:25%;vertical-align:middle">
                <img src="images/gaze2023_low_res.png", width="180"></td>
              <td width="75%" valign="center">
                <span style="font-size: 1.2em;"><strong>Invited poster and spotlight talk at <a href="https://gazeworkshop.github.io/2023/">CVPR GAZE2023 workshop</a>.</strong></span>
                <div style="margin-bottom: 0.2em;"></div> 
                <strong>Topic:</strong> Multi-person eyeblink detection in the wild in untrimmed videos.
                <br>
                
              </td>
            </tr>
        
            
            
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Competitions</h2>
                <br>
                (Team leader or core contributor) 
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>


            <tr>
              <td style="padding:5px 20px;width:25%;vertical-align:middle">
                <img src="images/logo_new.gif", width="180"></td>
              <td width="75%" valign="center">
                <!-- <span><strong>The "Challenge Cup" National Competition, 2023</strong></span> -->
                <span style="font-size: 1.2em;"><strong>The "Challenge Cup" National Competition, 2023</strong></span>
                <div style="margin-bottom: 0.2em;"></div>
      
                <strong>Topic:</strong> Driver Monitoring System (DMS).
                <br>
                <strong>Result:</strong> Winning the <strong>Grand Prize (top prize beyond First Prize), serving as team leader</strong>.
                <br>
                
              </td>
            </tr>
            
            <tr>
              <td style="padding:5px 20px;width:25%;vertical-align:middle">
                <img src="images/ECCV-logo3.png", width="150"></td>
              <td width="75%" valign="center">
                <!-- <span><strong>The Visual Inductive Priors for Data-Efficient Computer Vision Challenge, ECCV 2022</strong></span> -->
                <span style="font-size: 1.2em;"><strong>The Visual Inductive Priors for Data-Efficient Computer Vision Challenge, ECCV 2022</strong></span>
                <div style="margin-bottom: 0.2em;"></div>
                
                <strong>Topic:</strong> Action recognition with limited training data.
                <br>
                <strong>Result:</strong> Obtain <strong>3rd place</strong> and <strong>Jury Prize</strong> in the action recognition track (equal core contribution).
                <br>
                
              </td>
            </tr>


            <tr>
              <td style="padding:5px 20px;width:25%;vertical-align:middle">
                <img src="images/Parkinson_competition.jpg", width="150"></td>
              <td width="75%" valign="center">
                <!-- <span><strong>China Graduate AI Innovation Competition, 2021</strong></span> -->
                <span style="font-size: 1.2em;"><strong>China Graduate AI Innovation Competition, 2021</strong></span>
                <div style="margin-bottom: 0.2em;"></div>
                
                <strong>Topic:</strong> Cloud + AI assist Parkinson's Diagnosis.
                <br>
                <strong>Result:</strong> Obtain <strong>3rd place and fisrt prize (3/1505)</strong>.
                <br>
                
              </td>
            </tr>

            <tr>
              <td style="padding:5px 20px;width:25%;vertical-align:middle">
                <img src="images/ICCV logo_Homepage_sm.png", width="150"></td>
              <td width="75%" valign="center">
                <!-- <span><strong>Fisheye Video-based Action Recognition Competition at the MMVRAC Workshop, ICCV 2021</strong></span> -->
                <span style="font-size: 1.2em;"><strong>Fisheye Video-based Action Recognition Competition at the MMVRAC Workshop, ICCV 2021</strong></span>
                <div style="margin-bottom: 0.2em;"></div>
                
                <strong>Topic:</strong> Action recognition in fisheye videos.
                <br>
                <strong>Result:</strong> Obtain <strong>4th place</strong>.
                <br>
                
              </td>
            </tr>

            <tr>
              <td style="padding:5px 20px;width:25%;vertical-align:middle">
                <img src="images/COMAP-logo-EF3340.svg", width="150"></td>
              <td width="75%" valign="center">
                <!-- <span><strong>The Mathematical Contest In Modeling (MCM), America, 2020</strong></span> -->
                <span style="font-size: 1.15em;"><strong>The Mathematical Contest In Modeling (MCM), America, 2020</strong></span>
                <div style="margin-bottom: 0.2em;"></div>
                
                <strong>Topic:</strong> Comprehensive exploration of buyer reviews and ratings for Amazon e-commerce products.
                <br>
                <strong>Result:</strong> Obtain <strong>Meritorious Winner (9%)</strong> (equal core contribution).
                <br>
                
              </td>
            </tr>


            
          </tbody></table>


          <p></p><p></p><p></p><p></p><p></p>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                  <td width="100%" valign="middle">
                    <h2>Selected Awards</h2>
                    <div style="line-height:25px">
                    <p>
                <li> <stronghuge>PhD Research Scholarship, NUS,&nbsp; 2024<br/>
                <li> <stronghuge>PhD Fellowship, UCSD,&nbsp; 2024<br/>
                <li> <stronghuge>National Scholarship (Rank 1/600+),&nbsp; 2023<br/>
                <li> <stronghuge>HUAWEI Scholarship (Rate < 1%), &nbsp; 2023<br/>
                <li> <stronghuge>Outstanding Undergraduate,&nbsp; 2021<br/>
                <li> <stronghuge>Outstanding Undergraduate Thesis </stronghuge> (Rate < 3% in HUST),&nbsp; 2021<br/>
  
                    </p>
                    </div>
                  </td>
                </tr>
          </table>

          <table style="width:95%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=9uNQ2ofFYHm_dpMlwmA0ZP6JyNeR14BDIfGcfmwCp1A"></script>
              </td>
            </tr>
          </tbody></table>

          <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=9uNQ2ofFYHm_dpMlwmA0ZP6JyNeR14BDIfGcfmwCp1A&cl=ffffff&w=a"></script> -->

          <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=9uNQ2ofFYHm_dpMlwmA0ZP6JyNeR14BDIfGcfmwCp1A'></script> -->






          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This website is inspired by <a href="https://jonbarron.info/">Jon Barron's website</a>. Many thanks to him!
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
